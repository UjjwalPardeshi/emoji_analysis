{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcee8ba5-a8c3-4a5f-be06-e26d350f3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5524908d-4e5c-40e3-9c8f-d42ba8642530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"dataset/dataset.xlsx\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"final_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f0a9ca-2e8f-41e5-8f5e-1fcd55fe4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    description  score\n",
      "0                 grinning face   1.00\n",
      "1  smiling face with open mouth   1.00\n",
      "2                  winking face   1.00\n",
      "3                    robot face   0.50\n",
      "4              father christmas   0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from .parquet file\n",
    "df = pd.read_csv(\"dataset/final_dataset.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b982355d-9430-4723-8ca8-6366ab1d2e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2749 entries, 0 to 2748\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   description  2749 non-null   object \n",
      " 1   score        2749 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 43.1+ KB\n",
      "None\n",
      "             score\n",
      "count  2749.000000\n",
      "mean      0.636595\n",
      "std       0.233991\n",
      "min       0.000000\n",
      "25%       0.500000\n",
      "50%       0.500000\n",
      "75%       0.750000\n",
      "max       1.000000\n",
      "Index(['description', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.info())  # Summary of dataset\n",
    "print(df.describe())  # Statistics for numerical columns\n",
    "print(df.columns)  # List of column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e7cd9b-b21a-4207-a813-dfeaec31d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset/final_dataset.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Plot sentiment score distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[\"score\"].value_counts().sort_index().plot(kind=\"bar\", color=[\"red\", \"gray\", \"green\"])\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Emoji Sentiment Score Distribution\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e84b39e-9c59-4562-8c52-9edc5880c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score\n",
      "0.00      57\n",
      "0.25     180\n",
      "0.50    1228\n",
      "0.75     772\n",
      "1.00     512\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset/final_dataset.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Count the number of emojis for each sentiment score\n",
    "emoji_counts = df[\"score\"].value_counts().sort_index()\n",
    "\n",
    "# Display the counts\n",
    "print(emoji_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870a69ad-91fa-47bb-a73c-8876d8c6187a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>grinning face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>smiling face with open mouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>winking face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>robot face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>father christmas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>baby angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>baby angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>baby angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>baby angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>father christmas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  \\\n",
       "0     {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "1     {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "2     {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "3     {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "4     {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "...                                                 ...   \n",
       "2744  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "2745  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "2746  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "2747  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "2748  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
       "\n",
       "                              text  \n",
       "0                    grinning face  \n",
       "1     smiling face with open mouth  \n",
       "2                     winking face  \n",
       "3                       robot face  \n",
       "4                 father christmas  \n",
       "...                            ...  \n",
       "2744                    baby angel  \n",
       "2745                    baby angel  \n",
       "2746                    baby angel  \n",
       "2747                    baby angel  \n",
       "2748              father christmas  \n",
       "\n",
       "[2749 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(\"dataset/train-00000-of-00001-38cc4fa96c139e86.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e652a02f-b288-4089-af01-5b668e20458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['description', 'score'], dtype='object')\n",
      "Index(['image', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV dataset (which has sentiment scores)\n",
    "csv_df = pd.read_csv(\"dataset/final_dataset.csv\")\n",
    "\n",
    "# Load Parquet dataset (which has images)\n",
    "parquet_df = pd.read_parquet(\"dataset/train-00000-of-00001-38cc4fa96c139e86.parquet\")\n",
    "\n",
    "# Check column names\n",
    "print(csv_df.columns)\n",
    "print(parquet_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343ad9fe-c1dc-4e68-9bc0-667fb5393686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  \\\n",
      "0  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "1  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "2  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "3  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "4  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "\n",
      "                           text  score  \n",
      "0                 grinning face   1.00  \n",
      "1  smiling face with open mouth   1.00  \n",
      "2                  winking face   1.00  \n",
      "3                    robot face   0.50  \n",
      "4              father christmas   0.75  \n"
     ]
    }
   ],
   "source": [
    "# Merge CSV and Parquet datasets\n",
    "merged_df = pd.merge(parquet_df, csv_df, left_on=\"text\", right_on=\"description\", how=\"left\")\n",
    "\n",
    "# Drop duplicate 'description' column after merging\n",
    "merged_df.drop(columns=[\"description\"], inplace=True)\n",
    "\n",
    "# Show merged dataset\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4651a0-bdad-41d1-897f-67209db42ba4",
   "metadata": {},
   "source": [
    "# Convert Base64/Bytes Images to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbfa411-5baa-4e0d-90ce-52a4d0e568c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(merged_df[\"image\"].iloc[0]))  # Check the first image type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f71e87-d744-4da0-9772-88690829f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bytes', 'path'])\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[\"image\"].iloc[0].keys())  # See what keys are inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1136d4b-444b-401d-a7d1-6a786d87bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_data(image_dict):\n",
    "    \"\"\"Extracts image bytes from dictionary format.\"\"\"\n",
    "    if isinstance(image_dict, dict):\n",
    "        return image_dict.get(\"image_data\", None)  # Adjust key based on actual structure\n",
    "    return image_dict  # If already in correct format\n",
    "\n",
    "# Apply extraction\n",
    "merged_df[\"image\"] = merged_df[\"image\"].apply(extract_image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4aa773b-9410-4f6c-8cf8-a461524d4fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument should be a bytes-like object or ASCII string, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Apply transformation\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m merged_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecode_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to NumPy arrays\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(merged_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Image data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(image_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_data))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mbase64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb64decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))  \u001b[38;5;66;03m# Resize for CNN\u001b[39;00m\n\u001b[1;32m     14\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/base64.py:83\u001b[0m, in \u001b[0;36mb64decode\u001b[0;34m(s, altchars, validate)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mb64decode\u001b[39m(s, altchars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode the Base64 encoded bytes-like object or ASCII string s.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Optional altchars must be a bytes-like object or ASCII string of length 2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    https://docs.python.org/3.11/library/binascii.html#binascii.a2b_base64\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43m_bytes_from_decode_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m altchars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         altchars \u001b[38;5;241m=\u001b[39m _bytes_from_decode_data(altchars)\n",
      "File \u001b[0;32m/usr/lib64/python3.12/base64.py:45\u001b[0m, in \u001b[0;36m_bytes_from_decode_data\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(s)\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a bytes-like object or ASCII \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument should be a bytes-like object or ASCII string, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "def decode_image(image_data):\n",
    "    \"\"\"Convert base64/byte image to NumPy array.\"\"\"\n",
    "    if isinstance(image_data, bytes):\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "    else:\n",
    "        img = Image.open(io.BytesIO(base64.b64decode(image_data)))\n",
    "\n",
    "    img = img.resize((64, 64))  # Resize for CNN\n",
    "    img = np.array(img) / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Apply transformation\n",
    "merged_df[\"image\"] = merged_df[\"image\"].apply(decode_image)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.stack(merged_df[\"image\"].values)  # Image data\n",
    "y = merged_df[\"score\"].values            # Sentiment scores\n",
    "\n",
    "print(X.shape, y.shape)  # Verify dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f92627-ae24-46e5-9fdc-b3850945d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5869\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[\"image\"].isna().sum())  # Count missing images\n",
    "print(merged_df[\"image\"].dtype)  # Check the data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f328d4-c139-4e68-b914-2e75e4d2e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    None\n",
      "1    None\n",
      "2    None\n",
      "3    None\n",
      "4    None\n",
      "5    None\n",
      "6    None\n",
      "7    None\n",
      "8    None\n",
      "9    None\n",
      "Name: image, dtype: object\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print(merged_df[\"image\"].head(10))\n",
    "print(type(merged_df[\"image\"].iloc[0]))  # Check the first image type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ddcc0c-aeeb-4e20-b046-32650f189c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  \\\n",
      "0  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "1  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "2  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "3  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "4  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "5  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "6  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "7  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "8  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "9  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...   \n",
      "\n",
      "                           text  \n",
      "0                 grinning face  \n",
      "1  smiling face with open mouth  \n",
      "2                  winking face  \n",
      "3                    robot face  \n",
      "4              father christmas  \n",
      "5              father christmas  \n",
      "6              father christmas  \n",
      "7              father christmas  \n",
      "8              father christmas  \n",
      "9              mother christmas  \n",
      "image    object\n",
      "text     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(parquet_df.head(10))  # Check the first 10 rows of your Parquet dataset\n",
    "print(parquet_df.dtypes)    # Check the data types of all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72bb17f-f804-45c4-953f-f74a7864a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2749, 64, 64, 3), Labels Shape: (2749,)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Function to decode image bytes from dictionary\n",
    "def decode_image(image_dict):\n",
    "    if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(image_dict[\"bytes\"]))  # Open image from bytes\n",
    "            img = img.resize((64, 64))  # Resize for model compatibility\n",
    "            img = np.array(img) / 255.0  # Normalize pixel values (0-1)\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding image: {e}\")\n",
    "            return np.zeros((64, 64, 3))  # Return a blank image if decoding fails\n",
    "    return np.zeros((64, 64, 3))  # Handle missing images\n",
    "\n",
    "# Load Parquet dataset\n",
    "parquet_df = pd.read_parquet(\"dataset/train-00000-of-00001-38cc4fa96c139e86.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Apply function to extract images\n",
    "parquet_df[\"image\"] = parquet_df[\"image\"].apply(decode_image)\n",
    "\n",
    "# Convert images into a NumPy array for model training\n",
    "X = np.stack(parquet_df[\"image\"].values)  # Image data\n",
    "y = parquet_df[\"text\"].values             # Labels (emoji descriptions)\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}, Labels Shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07be3562-ead1-4566-a746-c341a0fcc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv(\"dataset/final_dataset.csv\")\n",
    "merged_df = pd.merge(parquet_df, csv_df, left_on=\"text\", right_on=\"description\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1961af63-fef8-4097-a47b-d12b3edf1537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (4695, 64, 64, 3), Testing Data: (1174, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to decode image bytes from dictionary\n",
    "def decode_image(image_dict):\n",
    "    if isinstance(image_dict, dict) and \"bytes\" in image_dict:\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(image_dict[\"bytes\"]))  # Open image\n",
    "            img = img.resize((64, 64))  # Resize for CNN\n",
    "            img = np.array(img) / 255.0  # Normalize (0-1)\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding image: {e}\")\n",
    "            return np.zeros((64, 64, 3))  # Blank image if error\n",
    "    return np.zeros((64, 64, 3))\n",
    "\n",
    "# Load datasets\n",
    "parquet_df = pd.read_parquet(\"dataset/train-00000-of-00001-38cc4fa96c139e86.parquet\", engine=\"pyarrow\")\n",
    "csv_df = pd.read_csv(\"dataset/final_dataset.csv\")\n",
    "\n",
    "# Merge datasets based on emoji description\n",
    "merged_df = pd.merge(parquet_df, csv_df, left_on=\"text\", right_on=\"description\", how=\"inner\")\n",
    "\n",
    "# Apply function to extract images\n",
    "merged_df[\"image\"] = merged_df[\"image\"].apply(decode_image)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.stack(merged_df[\"image\"].values)  # Image data\n",
    "y = merged_df[\"score\"].values            # Sentiment scores\n",
    "\n",
    "# One-hot encode sentiment scores\n",
    "y = to_categorical(y, num_classes=5)\n",
    "\n",
    "# Split dataset into training & testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Data: {X_train.shape}, Testing Data: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039d379-74e1-4653-a253-f3c491c558d4",
   "metadata": {},
   "source": [
    "# \n",
    "Since you have images with a size of (64, 64, 3), we’ll use a Convolutional Neural Network (CNN) with 3 convolutional layers and a softmax output layer for 5 sentiment classes (0, 0.25, 0.5, 0.75, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6aa30d-70ef-4ad8-a4ea-77e3e28438ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/ujjain/.local/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-12 22:39:09.755696: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m645\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,845</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m683,845\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,845</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,845\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation=\"softmax\")  # 5 sentiment classes\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d230bdd6-01ab-43cc-bb1f-fd5fbe292de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - accuracy: 0.8259 - loss: 0.4953 - val_accuracy: 0.9089 - val_loss: 0.2531\n",
      "Epoch 2/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.9139 - loss: 0.2326 - val_accuracy: 0.9225 - val_loss: 0.1865\n",
      "Epoch 3/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.9413 - loss: 0.1711 - val_accuracy: 0.9608 - val_loss: 0.1211\n",
      "Epoch 4/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.9543 - loss: 0.1284 - val_accuracy: 0.9676 - val_loss: 0.1089\n",
      "Epoch 5/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.9703 - loss: 0.0905 - val_accuracy: 0.9761 - val_loss: 0.0853\n",
      "Epoch 6/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9642 - loss: 0.0975 - val_accuracy: 0.9770 - val_loss: 0.0782\n",
      "Epoch 7/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9752 - loss: 0.0647 - val_accuracy: 0.9761 - val_loss: 0.0781\n",
      "Epoch 8/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9805 - loss: 0.0571 - val_accuracy: 0.9659 - val_loss: 0.1010\n",
      "Epoch 9/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9841 - loss: 0.0504 - val_accuracy: 0.9736 - val_loss: 0.0915\n",
      "Epoch 10/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9874 - loss: 0.0376 - val_accuracy: 0.9676 - val_loss: 0.0966\n",
      "Epoch 11/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9851 - loss: 0.0376 - val_accuracy: 0.9779 - val_loss: 0.0918\n",
      "Epoch 12/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9879 - loss: 0.0311 - val_accuracy: 0.9779 - val_loss: 0.1211\n",
      "Epoch 13/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9877 - loss: 0.0369 - val_accuracy: 0.9796 - val_loss: 0.1135\n",
      "Epoch 14/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9844 - loss: 0.0395 - val_accuracy: 0.9736 - val_loss: 0.1033\n",
      "Epoch 15/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9884 - loss: 0.0282 - val_accuracy: 0.9625 - val_loss: 0.1200\n",
      "Epoch 16/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9863 - loss: 0.0327 - val_accuracy: 0.9779 - val_loss: 0.1097\n",
      "Epoch 17/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9901 - loss: 0.0262 - val_accuracy: 0.9736 - val_loss: 0.0920\n",
      "Epoch 18/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9909 - loss: 0.0246 - val_accuracy: 0.9813 - val_loss: 0.0977\n",
      "Epoch 19/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9894 - loss: 0.0216 - val_accuracy: 0.9813 - val_loss: 0.1067\n",
      "Epoch 20/20\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9910 - loss: 0.0225 - val_accuracy: 0.9787 - val_loss: 0.1233\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9766 - loss: 0.1257\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Train CNN model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fad30c12-f8e7-4bd6-b454-982127eed9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"emoji_sentiment_model.h5\")  # Saves in HDF5 format\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f035710-dbe5-4208-a96d-636c4bbf9111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Predicted Sentiment: Negative\n",
      "Sentiment Scores: [2.3947980e-03 9.9760520e-01 2.0990184e-12 4.0451903e-13 3.6604408e-13]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"emoji_sentiment_model.h5\")  # Change filename if different\n",
    "\n",
    "def predict_emoji_sentiment(png_file, model):\n",
    "    \"\"\"Loads a PNG emoji image and predicts its sentiment using CNN.\"\"\"\n",
    "    try:\n",
    "        # Load image and preprocess\n",
    "        img = image.load_img(png_file, target_size=(64, 64))  # Resize\n",
    "        img_array = image.img_to_array(img) / 255.0  # Normalize (0-1)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict sentiment\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_index = np.argmax(predictions)  # Get highest probability class\n",
    "\n",
    "        # Define sentiment classes\n",
    "        sentiment_classes = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\n",
    "\n",
    "        return sentiment_classes[predicted_index], predictions[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "emoji_image = \"laughing.jpg\"  # Provide your PNG emoji file path\n",
    "sentiment_label, sentiment_scores = predict_emoji_sentiment(emoji_image, model)\n",
    "\n",
    "print(f\"Predicted Sentiment: {sentiment_label}\")\n",
    "print(f\"Sentiment Scores: {sentiment_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "839b553a-0aca-4eba-a013-7c9176d11b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating image from emoji: 'ImageDraw' object has no attribute 'textsize'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(emoji_text, model):\n",
    "    \"\"\"Converts a text emoji to an image and predicts sentiment using CNN model.\"\"\"\n",
    "    img = text_emoji_to_image(emoji_text)\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(img)\n",
    "    predicted_index = np.argmax(predictions)\n",
    "\n",
    "    # Define sentiment classes\n",
    "    sentiment_classes = [0, 0.25, 0.5, 0.75, 1]\n",
    "    return sentiment_classes[predicted_index]\n",
    "\n",
    "# Example usage:\n",
    "emoji_sentiment = predict_sentiment(\"😂\", model)\n",
    "print(\"Predicted Sentiment:\", emoji_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20e4bf86-04e1-4800-a76f-b6a22ca89db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42fbaa36-5f58-4bec-af0e-f8907c7dc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
    "    BatchNormalization(),  # Normalize activations\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    Dense(5, activation=\"softmax\")  # 5 sentiment classes\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d989b9aa-6c79-44ac-b79d-688d7226ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/ujjain/.local/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.7703 - loss: 1.1373 - val_accuracy: 0.8007 - val_loss: 0.4712\n",
      "Epoch 2/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 73ms/step - accuracy: 0.8587 - loss: 0.3889 - val_accuracy: 0.8578 - val_loss: 0.3655\n",
      "Epoch 3/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 76ms/step - accuracy: 0.8854 - loss: 0.3150 - val_accuracy: 0.8739 - val_loss: 0.3101\n",
      "Epoch 4/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 78ms/step - accuracy: 0.8767 - loss: 0.3294 - val_accuracy: 0.8271 - val_loss: 0.3773\n",
      "Epoch 5/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.8899 - loss: 0.2928 - val_accuracy: 0.9097 - val_loss: 0.2832\n",
      "Epoch 6/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.8882 - loss: 0.2671 - val_accuracy: 0.9029 - val_loss: 0.2506\n",
      "Epoch 7/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.8921 - loss: 0.2688 - val_accuracy: 0.9208 - val_loss: 0.2243\n",
      "Epoch 8/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 80ms/step - accuracy: 0.8924 - loss: 0.2604 - val_accuracy: 0.9319 - val_loss: 0.1836\n",
      "Epoch 9/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.8962 - loss: 0.2570 - val_accuracy: 0.9037 - val_loss: 0.2834\n",
      "Epoch 10/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9029 - loss: 0.2412 - val_accuracy: 0.9225 - val_loss: 0.1891\n",
      "Epoch 11/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9095 - loss: 0.2372 - val_accuracy: 0.9284 - val_loss: 0.2040\n",
      "Epoch 12/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9052 - loss: 0.2404 - val_accuracy: 0.9370 - val_loss: 0.1652\n",
      "Epoch 13/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9053 - loss: 0.2306 - val_accuracy: 0.9140 - val_loss: 0.2181\n",
      "Epoch 14/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9175 - loss: 0.2311 - val_accuracy: 0.9225 - val_loss: 0.1840\n",
      "Epoch 15/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9152 - loss: 0.2110 - val_accuracy: 0.9429 - val_loss: 0.1550\n",
      "Epoch 16/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 80ms/step - accuracy: 0.9191 - loss: 0.2044 - val_accuracy: 0.9259 - val_loss: 0.1850\n",
      "Epoch 17/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9189 - loss: 0.1876 - val_accuracy: 0.9055 - val_loss: 0.3399\n",
      "Epoch 18/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.9285 - loss: 0.1912 - val_accuracy: 0.9438 - val_loss: 0.1532\n",
      "Epoch 19/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.9290 - loss: 0.1922 - val_accuracy: 0.9020 - val_loss: 0.2455\n",
      "Epoch 20/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9265 - loss: 0.1769 - val_accuracy: 0.9523 - val_loss: 0.1652\n",
      "Epoch 21/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 80ms/step - accuracy: 0.9219 - loss: 0.2086 - val_accuracy: 0.9438 - val_loss: 0.1514\n",
      "Epoch 22/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9264 - loss: 0.1943 - val_accuracy: 0.9378 - val_loss: 0.1884\n",
      "Epoch 23/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.9254 - loss: 0.1873 - val_accuracy: 0.8296 - val_loss: 0.4165\n",
      "Epoch 24/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 82ms/step - accuracy: 0.9263 - loss: 0.2053 - val_accuracy: 0.7956 - val_loss: 0.4402\n",
      "Epoch 25/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9245 - loss: 0.1995 - val_accuracy: 0.8441 - val_loss: 0.3818\n",
      "Epoch 26/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9349 - loss: 0.1681 - val_accuracy: 0.9438 - val_loss: 0.1399\n",
      "Epoch 27/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 84ms/step - accuracy: 0.9293 - loss: 0.1708 - val_accuracy: 0.9489 - val_loss: 0.1191\n",
      "Epoch 28/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9427 - loss: 0.1558 - val_accuracy: 0.8978 - val_loss: 0.4571\n",
      "Epoch 29/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 84ms/step - accuracy: 0.9438 - loss: 0.1609 - val_accuracy: 0.9514 - val_loss: 0.1381\n",
      "Epoch 30/30\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.9374 - loss: 0.1608 - val_accuracy: 0.8288 - val_loss: 0.4485\n"
     ]
    }
   ],
   "source": [
    "# Train using augmented data\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feec260c-278b-4cfc-acde-90f692ac1288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"emoji_image_model.h5\")  # Saves in HDF5 format\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66194d51-2fe8-48ea-9e33-b0b86e26fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7c40087b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7c40087b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Average Sentiment Score: 0.2000\n",
      "Sentiment Scores: [9.4963270e-01 5.0367337e-02 1.0063820e-13 3.0436198e-20 1.7318827e-19]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"emoji_image_model.h5\")  # Change filename if different\n",
    "\n",
    "def predict_emoji_sentiment(png_file, model):\n",
    "    \"\"\"Loads a PNG emoji image and predicts sentiment scores using CNN.\"\"\"\n",
    "    try:\n",
    "        # Load image and preprocess\n",
    "        img = image.load_img(png_file, target_size=(64, 64))  # Resize\n",
    "        img_array = image.img_to_array(img) / 255.0  # Normalize (0-1)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict sentiment scores\n",
    "        predictions = model.predict(img_array)  # Array of probabilities\n",
    "        avg_sentiment_score = np.mean(predictions[0])  # Compute average score\n",
    "\n",
    "        return avg_sentiment_score, predictions[0]  # Return avg score & raw scores\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "emoji_image = \"heart_image.png\"  # Provide your PNG emoji file path\n",
    "avg_score, sentiment_scores = predict_emoji_sentiment(emoji_image, model)\n",
    "\n",
    "print(f\"Average Sentiment Score: {avg_score:.4f}\")\n",
    "print(f\"Sentiment Scores: {sentiment_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56002292-115c-46fc-9d73-a4057065a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating image: 'ImageDraw' object has no attribute 'textsize'\n",
      "Error: Could not generate emoji image.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m emoji_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m😂\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Provide emoji as text\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m avg_score, sentiment_scores \u001b[38;5;241m=\u001b[39m predict_emoji_sentiment_from_text(emoji_text, model)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Sentiment Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"emoji_image_model.h5\")  # Change filename if different\n",
    "\n",
    "def text_to_image(emoji_text, image_size=(64, 64), font_size=50):\n",
    "    \"\"\"Converts an emoji text to an image.\"\"\"\n",
    "    try:\n",
    "        # Create blank image (white background)\n",
    "        img = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Load font (adjust path if needed)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)  # Windows\n",
    "        except:\n",
    "            font = ImageFont.load_default()  # Use default if font not found\n",
    "\n",
    "        # Get text size and center it\n",
    "        text_width, text_height = draw.textsize(emoji_text, font=font)\n",
    "        position = ((image_size[0] - text_width) // 2, (image_size[1] - text_height) // 2)\n",
    "\n",
    "        # Draw emoji text\n",
    "        draw.text(position, emoji_text, fill=(0, 0, 0), font=font)\n",
    "\n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating image: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_emoji_sentiment_from_text(emoji_text, model):\n",
    "    \"\"\"Converts emoji text to image, preprocesses it, and predicts sentiment.\"\"\"\n",
    "    # Convert text emoji to image\n",
    "    img = text_to_image(emoji_text)\n",
    "\n",
    "    if img is None:\n",
    "        print(\"Error: Could not generate emoji image.\")\n",
    "        return None\n",
    "\n",
    "    # Preprocess image for model\n",
    "    img = img.resize((64, 64))  # Resize\n",
    "    img_array = image.img_to_array(img) / 255.0  # Normalize (0-1)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict sentiment scores\n",
    "    predictions = model.predict(img_array)\n",
    "    avg_sentiment_score = np.mean(predictions[0])  # Compute average score\n",
    "\n",
    "    return avg_sentiment_score, predictions[0]\n",
    "\n",
    "# Example usage:\n",
    "emoji_text = \"😂\"  # Provide emoji as text\n",
    "avg_score, sentiment_scores = predict_emoji_sentiment_from_text(emoji_text, model)\n",
    "\n",
    "print(f\"Average Sentiment Score: {avg_score:.4f}\")\n",
    "print(f\"Sentiment Scores: {sentiment_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b4346-af95-46bb-b08f-653ed51957b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
